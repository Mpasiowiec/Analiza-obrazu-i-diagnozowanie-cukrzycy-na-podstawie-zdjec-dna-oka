{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ternausnet > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ternausnet.models\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(url, filepath):\n",
    "    directory = os.path.dirname(os.path.abspath(filepath))\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    if os.path.exists(filepath):\n",
    "        print(\"Dataset already exists on the disk. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    with TqdmUpTo(unit=\"B\", unit_scale=True, unit_divisor=1024, miniters=1, desc=os.path.basename(filepath)) as t:\n",
    "        urlretrieve(url, filename=filepath, reporthook=t.update_to, data=None)\n",
    "        t.total = t.n\n",
    "\n",
    "\n",
    "def extract_archive(filepath):\n",
    "    extract_dir = os.path.dirname(os.path.abspath(filepath))\n",
    "    shutil.unpack_archive(filepath, extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = os.path.join(os.environ[\"HOME\"], \"datasets/oxford-iiit-pet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(dataset_directory, \"images.tar.gz\")\n",
    "download_url(\n",
    "    url=\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\", filepath=filepath,\n",
    ")\n",
    "extract_archive(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(dataset_directory, \"annotations.tar.gz\")\n",
    "download_url(\n",
    "    url=\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\", filepath=filepath,\n",
    ")\n",
    "extract_archive(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = os.path.join(dataset_directory)\n",
    "images_directory = os.path.join(root_directory, \"images\")\n",
    "masks_directory = os.path.join(root_directory, \"annotations\", \"trimaps\")\n",
    "\n",
    "images_filenames = list(sorted(os.listdir(images_directory)))\n",
    "correct_images_filenames = [i for i in images_filenames if cv2.imread(os.path.join(images_directory, i)) is not None]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(correct_images_filenames)\n",
    "\n",
    "train_images_filenames = correct_images_filenames[:6000]\n",
    "val_images_filenames = correct_images_filenames[6000:-10]\n",
    "test_images_filenames = images_filenames[-10:]\n",
    "\n",
    "print(len(train_images_filenames), len(val_images_filenames), len(test_images_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mask(mask):\n",
    "    mask = mask.astype(np.float32)\n",
    "    mask[mask == 2.0] = 0.0\n",
    "    mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(images_filenames, images_directory, masks_directory, predicted_masks=None):\n",
    "    cols = 3 if predicted_masks else 2\n",
    "    rows = len(images_filenames)\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 24))\n",
    "    for i, image_filename in enumerate(images_filenames):\n",
    "        image = cv2.imread(os.path.join(images_directory, image_filename))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,)\n",
    "        mask = preprocess_mask(mask)\n",
    "        ax[i, 0].imshow(image)\n",
    "        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n",
    "\n",
    "        ax[i, 0].set_title(\"Image\")\n",
    "        ax[i, 1].set_title(\"Ground truth mask\")\n",
    "\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "\n",
    "        if predicted_masks:\n",
    "            predicted_mask = predicted_masks[i]\n",
    "            ax[i, 2].imshow(predicted_mask, interpolation=\"nearest\")\n",
    "            ax[i, 2].set_title(\"Predicted mask\")\n",
    "            ax[i, 2].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(test_images_filenames, images_directory, masks_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image_filename = correct_images_filenames[0]\n",
    "image = cv2.imread(os.path.join(images_directory, example_image_filename))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "resized_image = F.resize(image, height=256, width=256)\n",
    "padded_image = F.pad(image, min_height=512, min_width=512)\n",
    "padded_constant_image = F.pad(image, min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT)\n",
    "cropped_image = F.center_crop(image, crop_height=256, crop_width=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(nrows=1, ncols=5, figsize=(18, 10))\n",
    "ax.ravel()[0].imshow(image)\n",
    "ax.ravel()[0].set_title(\"Original image\")\n",
    "ax.ravel()[1].imshow(resized_image)\n",
    "ax.ravel()[1].set_title(\"Resized image\")\n",
    "ax.ravel()[2].imshow(cropped_image)\n",
    "ax.ravel()[2].set_title(\"Cropped image\")\n",
    "ax.ravel()[3].imshow(padded_image)\n",
    "ax.ravel()[3].set_title(\"Image padded with reflection\")\n",
    "ax.ravel()[4].imshow(padded_constant_image)\n",
    "ax.ravel()[4].set_title(\"Image padded with constant padding\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(\n",
    "            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,\n",
    "        )\n",
    "        mask = preprocess_mask(mask)\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(256, 256),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [A.Resize(256, 256), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",
    ")\n",
    "val_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, idx=0, samples=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    figure, ax = plt.subplots(nrows=samples, ncols=2, figsize=(10, 24))\n",
    "    for i in range(samples):\n",
    "        image, mask = dataset[idx]\n",
    "        ax[i, 0].imshow(image)\n",
    "        ax[i, 1].imshow(mask, interpolation=\"nearest\")\n",
    "        ax[i, 0].set_title(\"Augmented image\")\n",
    "        ax[i, 1].set_title(\"Augmented mask\")\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "visualize_augmentations(train_dataset, idx=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, params):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    stream = tqdm(train_loader)\n",
    "    for i, (images, target) in enumerate(stream, start=1):\n",
    "        images = images.to(params[\"device\"], non_blocking=True)\n",
    "        target = target.to(params[\"device\"], non_blocking=True)\n",
    "        output = model(images).squeeze(1)\n",
    "        loss = criterion(output, target)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stream.set_description(\n",
    "            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)\n",
    "            output = model(images).squeeze(1)\n",
    "            loss = criterion(output, target)\n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            stream.set_description(\n",
    "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    model = getattr(ternausnet.models, params[\"model\"])(pretrained=True)\n",
    "    model = model.to(params[\"device\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_dataset, val_dataset, params):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=params[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss().to(params[\"device\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    for epoch in range(1, params[\"epochs\"] + 1):\n",
    "        train(train_loader, model, criterion, optimizer, epoch, params)\n",
    "        validate(val_loader, model, criterion, epoch, params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, params, test_dataset, batch_size):\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, (original_heights, original_widths) in test_loader:\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            output = model(images)\n",
    "            probabilities = torch.sigmoid(output.squeeze(1))\n",
    "            predicted_masks = (probabilities >= 0.5).float() * 1\n",
    "            predicted_masks = predicted_masks.cpu().numpy()\n",
    "            for predicted_mask, original_height, original_width in zip(\n",
    "                predicted_masks, original_heights.numpy(), original_widths.numpy()\n",
    "            ):\n",
    "                predictions.append((predicted_mask, original_height, original_width))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"model\": \"UNet11\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(params)\n",
    "model = train_and_validate(model, train_dataset, val_dataset, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPetInferenceDataset(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, transform=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        original_size = tuple(image.shape[:2])\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "        return image, original_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [A.Resize(256, 256), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()]\n",
    ")\n",
    "test_dataset = OxfordPetInferenceDataset(test_images_filenames, images_directory, transform=test_transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, params, test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = []\n",
    "for predicted_256x256_mask, original_height, original_width in predictions:\n",
    "    full_sized_mask = F.resize(\n",
    "        predicted_256x256_mask, height=original_height, width=original_width, interpolation=cv2.INTER_NEAREST\n",
    "    )\n",
    "    predicted_masks.append(full_sized_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(test_images_filenames, images_directory, masks_directory, predicted_masks=predicted_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=256, min_width=256),\n",
    "        A.RandomCrop(256, 256),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=256, min_width=256),\n",
    "        A.CenterCrop(256, 256),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"model\": \"UNet11\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(params)\n",
    "model = train_and_validate(model, train_dataset, val_dataset, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "test_dataset = OxfordPetInferenceDataset(test_images_filenames, images_directory, transform=test_transform,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, params, test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = []\n",
    "for predicted_padded_mask, original_height, original_width in predictions:\n",
    "    cropped_mask = F.center_crop(predicted_padded_mask, original_height, original_width)\n",
    "    predicted_masks.append(cropped_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(test_images_filenames, images_directory, masks_directory, predicted_masks=predicted_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(512),\n",
    "        A.PadIfNeeded(min_height=512, min_width=512),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(512),\n",
    "        A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"model\": \"UNet11\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(params)\n",
    "model = train_and_validate(model, train_dataset, val_dataset, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "test_dataset = OxfordPetInferenceDataset(test_images_filenames, images_directory, transform=test_transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, params, test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = []\n",
    "for predicted_padded_mask, original_height, original_width in predictions:\n",
    "    cropped_mask = F.center_crop(predicted_padded_mask, original_height, original_width)\n",
    "    predicted_masks.append(cropped_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(test_images_filenames, images_directory, masks_directory, predicted_masks=predicted_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
